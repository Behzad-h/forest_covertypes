{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training for the Forest Covertypes Data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://cdn.shopify.com/s/files/1/0326/7189/t/65/assets/pf-85b5b49e--Website-Header-2000px-x-600px.jpg?v=1625226604)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Image Credit](https://onetreeplanted.org/pages/million-tree-challenge) <br>\n",
    "\n",
    "In this notebook, the data of **Forest Cover Type Prediction** is used to develop prediction models. <br>\n",
    "The data can be downloaded from: <br>\n",
    "https://www.kaggle.com/competitions/forest-cover-type-prediction/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "import os\n",
    "\n",
    "# to bypass warnings in various dataframe assignments\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "pd.set_option('display.max_columns', 60)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train = pd.read_csv(\"data/processed/train.csv\")\n",
    "test = pd.read_csv(\"data/processed/test.csv\")\n",
    "\n",
    "sample_submission = pd.read_csv(\"data/raw/sampleSubmission.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Feature-Label:**\n",
    "The first thing we need to do is separating the label from the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into features and labels\n",
    "x_train = train.drop(['Cover_Type','Id'], axis=1)\n",
    "y_train = train['Cover_Type']\n",
    "\n",
    "x_test = test.drop(['Id'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to implement cross-validation and considering that we already have a blind test set, we do not split the training set to train-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training input:  (14988, 54)\n",
      "Shape of training labels:  (14988,)\n",
      "Shape of test input:  (565892, 54)\n"
     ]
    }
   ],
   "source": [
    "# checking the shape of the data\n",
    "print(\"Shape of training input: \", x_train.shape)\n",
    "print(\"Shape of training labels: \", y_train.shape)\n",
    "print(\"Shape of test input: \", x_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Creating Baseline Model:**\n",
    "In this section, we create a baseline model which is used as a benchmark to compare the performance of the other models. <br>\n",
    "This model is usually a simple method of predicting labels based on the relationships between the features and the target. If there are obvious relationships, we can make predictions based on that. If we have access to an expert who is aware of such relationships based on the underlying technical principles, we can code it to arrive at some predictions. <br>\n",
    "Considering the lack of the above knwoledge, we make the baseline predictions based on a stratified selection from the target classes using `DummyClassifier` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the dummy algorithm is 0.141 over the training set.\n"
     ]
    }
   ],
   "source": [
    "# Create dummy classifer\n",
    "dummy = DummyClassifier(strategy='stratified', random_state=1)\n",
    "\n",
    "# train the model\n",
    "dummy.fit(x_train, y_train)\n",
    "\n",
    "# accuracy score of the model on the training set\n",
    "accuracy_dm = dummy.score(x_train, y_train)\n",
    "print(f'The accuracy of the dummy algorithm is {accuracy_dm.round(3)} over the training set.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the dummy classifier could predict only 13.5 % of the labels correctly in the validation set. <br>\n",
    "It's time to try other classifiers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Developing Prediction Models Using All Features:**\n",
    "In this section, we develop several machine learning models to predict the cover types in the validation set. <br>\n",
    "These models are:\n",
    "* Logistic Regression\n",
    "* K-Nearest Neighbors\n",
    "* Decision Tree\n",
    "* Gradient Boosting Tree\n",
    "* Random Forest\n",
    "* Extra Trees (Extreme Random Forest)\n",
    "\n",
    "In order to make sure the best models will be fitted on the data, 5-fold cross validation together with hyperparameters tuning based on Bayesian optimization is applied to all the models. <br>\n",
    "\n",
    "**Standardizing data before training?** <br>\n",
    "\n",
    "For some algorithms, it is very important to scale data before training the models. This can significantly affect the performance of the models. Some comparisons between the performance of models developed using scaled data and unscaled data can be found [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html). It states that:\n",
    "\n",
    "> Even if tree based models are (almost) not affected by scaling, many other algorithms require features to be normalized, often for different reasons: to ease the convergence (such as a non-penalized logistic regression), to create a completely different model fit compared to the fit with unscaled data (such as KNeighbors models). The latter is demoed on the first part of the present example.\n",
    "\n",
    "So, we first standardize the data based on Z-score normalization as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training input:  (14988, 54)\n",
      "Shape of test input:  (565892, 54)\n"
     ]
    }
   ],
   "source": [
    "# standardize the train, validation, and test data all based on the training data excluding binary features\n",
    "binary_features = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', \n",
    "'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13',\n",
    " 'Soil_Type14', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', \n",
    " 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', \n",
    " 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.drop(binary_features, axis=1))\n",
    "x_test_scaled = scaler.transform(x_test.drop(binary_features, axis=1))\n",
    "\n",
    "# convert the numpy arrays to dataframes\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled, columns=x_train.drop(binary_features, axis=1).columns)\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled, columns=x_test.drop(binary_features, axis=1).columns)\n",
    "\n",
    "# add the binary features back to the dataframes\n",
    "x_train_scaled = pd.concat((x_train_scaled, x_train[binary_features].reset_index(drop=True)), axis=1)\n",
    "x_test_scaled = pd.concat((x_test_scaled, x_test[binary_features].reset_index(drop=True)), axis=1)\n",
    "\n",
    "# check the shape of the data\n",
    "print(\"Shape of training input: \", x_train_scaled.shape)\n",
    "print(\"Shape of test input: \", x_test_scaled.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Model 1: Logistic Regression**\n",
    "The first model we are going to develop is logistic regression using `LogisticRegression` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=5, random_state=1),\n",
       "              estimator=LogisticRegression(multi_class='multinomial',\n",
       "                                           solver='newton-cg'),\n",
       "              n_jobs=-1, random_state=1, return_train_score=True,\n",
       "              search_spaces={'C': Real(low=0.001, high=100, prior='log-uniform', transform='normalize'),\n",
       "                             'fit_intercept': [False, True]})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "param_grid = {'fit_intercept' : [False, True],\n",
    "              'C'             : Real(1e-3, 100, prior='log-uniform')}\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "search1 = BayesSearchCV(estimator=model, search_spaces=param_grid, cv=cv, n_jobs=-1, return_train_score=True, random_state=1, verbose=0)\n",
    "search1.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the average CV scores and the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameters of the model = OrderedDict([('C', 2.7502092219694063), ('fit_intercept', True)])\n",
      "CV AVG train score of the model = 0.7137\n",
      "CV AVG validation score of the model = 0.7081\n"
     ]
    }
   ],
   "source": [
    "accuracy_lr_cv_train = search1.cv_results_['mean_train_score'][search1.best_index_].round(4)\n",
    "accuracy_lr_cv_val = search1.cv_results_['mean_test_score'][search1.best_index_].round(4)\n",
    "\n",
    "print(f\"Optimal hyperparameters of the model = {search1.best_params_}\")\n",
    "print(f\"CV AVG train score of the model = {accuracy_lr_cv_train}\")\n",
    "print(f\"CV AVG validation score of the model = {accuracy_lr_cv_val}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to build model with the optimal hyperparameters using the whole training data (without cross-validation). Then, make final predictions on the blind test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model over the training data is 0.713.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(565892,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with optimal hyperparameters on the entire training data\n",
    "model1 = LogisticRegression(solver='newton-cg', multi_class='multinomial', **search1.best_params_)\n",
    "model1.fit(x_train_scaled, y_train)\n",
    "\n",
    "# check the accuracy of the model on the training data itself\n",
    "y_pred = model1.predict(x_train_scaled)\n",
    "accuracy_lr_train = accuracy_score(y_train, y_pred)\n",
    "print(f'The accuracy of the model over the training data is {accuracy_lr_train.round(3)}.')\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = model1.predict(x_test_scaled)\n",
    "y_pred.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a csv file of the predictions to submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    201932\n",
       "2    187825\n",
       "5     67134\n",
       "7     43203\n",
       "6     32439\n",
       "3     27185\n",
       "4      6174\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1 = sample_submission.copy()\n",
    "submission1['Cover_Type'] = y_pred\n",
    "\n",
    "# Save the submission file\n",
    "submission1.to_csv('data/prediction/submission1.csv', index=False)\n",
    "\n",
    "# Check the submission file\n",
    "submission1['Cover_Type'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Scores: CV train = 0.7137 | CV validation = 0.7081 | test (blind): 0.597**\n",
    "__________\n",
    "__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Model 2: K-Nearest Neighbors**\n",
    "The second model we are going to develop is k-nearest neighbors using `KNeighborsClassifier` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=5, random_state=1),\n",
       "              estimator=KNeighborsClassifier(), n_iter=10, n_jobs=-1,\n",
       "              random_state=1, return_train_score=True,\n",
       "              search_spaces={'n_neighbors': Integer(low=1, high=40, prior='uniform', transform='normalize'),\n",
       "                             'weights': Categorical(categories=('uniform', 'distance'), prior=None)})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors' : Integer(1,40), \n",
    "              'weights'     : Categorical(['uniform', 'distance'])}\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "search2 = BayesSearchCV(estimator=model, search_spaces=param_grid, n_iter=10, cv=cv, n_jobs=-1, return_train_score=True, random_state=1, verbose=0)\n",
    "search2.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the average CV scores and the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameters of the model = OrderedDict([('n_neighbors', 6), ('weights', 'uniform')])\n",
      "CV AVG train score of the model = 0.8493\n",
      "CV AVG validation score of the model = 0.7846\n"
     ]
    }
   ],
   "source": [
    "accuracy_knn_cv_train = search2.cv_results_['mean_train_score'][search2.best_index_].round(4)\n",
    "accuracy_knn_cv_val = search2.cv_results_['mean_test_score'][search2.best_index_].round(4)\n",
    "\n",
    "print(f\"Optimal hyperparameters of the model = {search2.best_params_}\")\n",
    "print(f\"CV AVG train score of the model = {accuracy_knn_cv_train}\")\n",
    "print(f\"CV AVG validation score of the model = {accuracy_knn_cv_val}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to build model with the optimal hyperparameters using the whole training data (without cross-validation). Then, make final predictions on the blind test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MIO\\.conda\\envs\\envtf\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model over the training data is 0.858.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MIO\\.conda\\envs\\envtf\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(565892,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with optimal hyperparameters on the entire training data\n",
    "model2 = KNeighborsClassifier(**search2.best_params_)\n",
    "model2.fit(x_train_scaled, y_train)\n",
    "\n",
    "# check the accuracy of the model on the training data itself\n",
    "y_pred = model2.predict(x_train_scaled)\n",
    "accuracy_knn_train = accuracy_score(y_train, y_pred)\n",
    "print(f'The accuracy of the model over the training data is {accuracy_knn_train.round(3)}.')\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = model2.predict(x_test_scaled)\n",
    "y_pred.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a csv file of the predictions to submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    217106\n",
       "2    187152\n",
       "5     55709\n",
       "7     37077\n",
       "3     35520\n",
       "6     28654\n",
       "4      4674\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission2 = sample_submission.copy()\n",
    "submission2['Cover_Type'] = y_pred\n",
    "\n",
    "# Save the submission file\n",
    "submission2.to_csv('data/prediction/submission2.csv', index=False)\n",
    "\n",
    "# Check the submission file\n",
    "submission2['Cover_Type'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Scores: CV train = 0.8493 | CV validation = 0.7846 | test (blind): 0.643**\n",
    "__________\n",
    "__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Model 3: Decision Tree**\n",
    "The third model we are going to develop is decision tree using `DecisionTreeClassifier` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=5, random_state=1),\n",
       "              estimator=DecisionTreeClassifier(), n_iter=10, n_jobs=-1,\n",
       "              random_state=1, return_train_score=True,\n",
       "              search_spaces={'max_depth': Integer(low=1, high=30, prior='uniform', transform='normalize')})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth':Integer(1,30)}\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "search3 = BayesSearchCV(estimator=model, search_spaces=param_grid, n_iter=10, cv=cv, n_jobs=-1, return_train_score=True, random_state=1, verbose=0)\n",
    "search3.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the average CV scores and the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameters of the model = OrderedDict([('max_depth', 21)])\n",
      "CV AVG train score of the model = 0.9935\n",
      "CV AVG validation score of the model = 0.7971\n"
     ]
    }
   ],
   "source": [
    "accuracy_dt_cv_train = search3.cv_results_['mean_train_score'][search3.best_index_].round(4)\n",
    "accuracy_dt_cv_val = search3.cv_results_['mean_test_score'][search3.best_index_].round(4)\n",
    "\n",
    "print(f\"Optimal hyperparameters of the model = {search3.best_params_}\")\n",
    "print(f\"CV AVG train score of the model = {accuracy_dt_cv_train}\")\n",
    "print(f\"CV AVG validation score of the model = {accuracy_dt_cv_val}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to build model with the optimal hyperparameters using the whole training data (without cross-validation). Then, make final predictions on the blind test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model over the training data is 0.994.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(565892,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with optimal hyperparameters on the entire training data\n",
    "model3 = DecisionTreeClassifier(**search3.best_params_)\n",
    "model3.fit(x_train_scaled, y_train)\n",
    "\n",
    "# check the accuracy of the model on the training data itself\n",
    "y_pred = model3.predict(x_train_scaled)\n",
    "accuracy_dt_train = accuracy_score(y_train, y_pred)\n",
    "print(f'The accuracy of the model over the training data is {accuracy_dt_train.round(3)}.')\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = model3.predict(x_test_scaled)\n",
    "y_pred.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a csv file of the predictions to submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    223022\n",
       "1    206127\n",
       "5     38723\n",
       "3     35546\n",
       "7     34382\n",
       "6     25428\n",
       "4      2664\n",
       "Name: Cover_Type, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission3 = sample_submission.copy()\n",
    "submission3['Cover_Type'] = y_pred\n",
    "\n",
    "# Save the submission file\n",
    "submission3.to_csv('data/prediction/submission3.csv', index=False)\n",
    "\n",
    "# Check the submission file\n",
    "submission3['Cover_Type'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Scores: CV train = 0.9935 | CV validation = 0.7971 | test (blind): 0.672**\n",
    "__________\n",
    "__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 Model 4: Gradient Boosting Tree**\n",
    "The fourth model we are going to develop is gradient boosting tree using `GradientBoostingClassifier` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled.iloc[:,:12].info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ddf6f5be8f1192cf70682edcaa6aaad42b4d7dc59537661b95caff2b632d806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
